PCA takes a dataset with many variables(features) and finds a smaller set of new artificial variables called principal components. This new components are ordered by how much of the original data's variation they capture in this way we can discribe the data very well with just few principal components instead of many original variables. Thes Steps of PCA are:

1- Data Standrization which is simply putting alll the features on the same scale by making the new average to be zero and the standard deviation to be 1.
2- Covariance Matrix is how the variables relate to each other.
3- Eigenvectors are the directions of the axes of the shape defining with the data points, so these are our new principal components. they are the directions that best summarize the data's spread.
4- Eigenvalues tell how much variance there is along each eigenvector. A high eigenvalues means there is a lot of variation in the data along the eigenvector's direction.
5- Sorting Eigenvectors and Eigenvalues from the most important to least important. so in this way we will have an ordered list of principal components that tell which directions in data are the most significant.
6- Choosing the # of principal components 1st we will look at the explained variance for each principal component its eigenvalue diveded by the sum of all eigenvalues and that will tell the percentage of the total variance or the cumulative explained variance. for example we will find the 1st comp explain the 70% of the total variance and the 2nd component explains 20% so both expalins 90%, at this high level of percentage we can decide to keep just those 2 components.
7- projecting the data, it is the transforming the the original new data into this new simplified space defiend by the principal components. projecting the data gives a new dataset with fewer columns, but which still captures the essence of the original data.

The geometry of projection a data from a cloud to a shadow, imagine your original image data for the digits '0', '1' and '8'. Each image has 784 pixels(28*28), so we can think for each image as a single point in a massive 784-dimensional space. It's impossible to visualize, but's lets pretend we can. The points for all the '0' will likley b clustered togther in one region of this space. the '1's another and the '8's in a third. all togther they form a "cloud" of data points in this high dimensional space. Now recall the 1st eigenvector (principal component 1) is the direction that acts through the center of the entire data cloud in the way that shows the most possible spread or variance. the 2nd eigenvector (principal component 2) is the next direction that captures the most remaining spread, while being orthogonal to the 1st one. when projecting this dataset onto these 2 eigenvectors we are essentially taking the high diemnsional data cloud and casting its shadow onto a flat surface(2D plane) defiened by these 2 new axes. 
The magic of PCA is that it doesn't just pick any random flat surface to cast the shadow on, it finds the perfect surface that keeps the data points as spread out as possible.
Thinking about the shapes of the digitis, '1's are thin anf tall, the pixels that are on are mostely vertical line. '0's are rounded, the on pixels form a loop, which is a very different pattern of variation. and the '8's are like two loops.
PCA finds the directions (the eigenvectors) that best capture these fundemental differencs. PCA finds a new basis is the set of the eigenvectors(the principal components) instead of describing an image by its pixels values, we can now decribe it by its 'Scores' along these new components axes. for example we can describe the dataset as 90% of principal component 1("the vertical-ness") and the component 2 ("the loop-ness) which is more efficient basis because it's ordered by importance and it's compact. 

